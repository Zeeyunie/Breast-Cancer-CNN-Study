{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dicom_info and building dictionaries...\n",
      "  full_mammo_dict: 2857 entries\n",
      "  cropped_images_dict: 3567 entries\n",
      "  roi_img_dict: 3247 entries\n",
      "Found case description files to process:\n",
      "  - calc_case_description_test_set.csv\n",
      "  - calc_case_description_train_set.csv\n",
      "  - mass_case_description_test_set.csv\n",
      "  - mass_case_description_train_set.csv\n",
      "\n",
      "Processing calc_case_description_test_set.csv...\n",
      "  Original shape: (326, 14)\n",
      "  Added label column\n",
      "  Saved cleaned file: calc_case_description_test_set_cleaned.csv\n",
      "  Final shape: (326, 15)\n",
      "\n",
      "Processing calc_case_description_train_set.csv...\n",
      "  Original shape: (1546, 14)\n",
      "  Added label column\n",
      "  Saved cleaned file: calc_case_description_train_set_cleaned.csv\n",
      "  Final shape: (1546, 15)\n",
      "\n",
      "Processing mass_case_description_test_set.csv...\n",
      "  Original shape: (378, 14)\n",
      "  Added label column\n",
      "  Saved cleaned file: mass_case_description_test_set_cleaned.csv\n",
      "  Final shape: (378, 15)\n",
      "\n",
      "Processing mass_case_description_train_set.csv...\n",
      "  Original shape: (1318, 14)\n",
      "  Added label column\n",
      "  Saved cleaned file: mass_case_description_train_set_cleaned.csv\n",
      "  Final shape: (1318, 15)\n",
      "\n",
      "All files processed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rando\\AppData\\Local\\Temp\\ipykernel_15808\\3616774029.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['pathology'].replace(label_map)\n",
      "C:\\Users\\rando\\AppData\\Local\\Temp\\ipykernel_15808\\3616774029.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['pathology'].replace(label_map)\n",
      "C:\\Users\\rando\\AppData\\Local\\Temp\\ipykernel_15808\\3616774029.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['pathology'].replace(label_map)\n",
      "C:\\Users\\rando\\AppData\\Local\\Temp\\ipykernel_15808\\3616774029.py:107: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['pathology'].replace(label_map)\n"
     ]
    }
   ],
   "source": [
    "# Build dictionaries and process all CSV files\n",
    "\n",
    "csv_folder = \"./cbis-ddsm-breast-cancer-image-dataset/csv\"\n",
    "indir = \"./cbis-ddsm-breast-cancer-image-dataset/jpeg\"\n",
    "\n",
    "# Robust solution derived from Midterms Prototype to solve file error\n",
    "# File error persisted in calc_case_description_test_set.csv\n",
    "# Load dicom_info and build UID->path dictionaries\n",
    "print(\"Loading dicom_info and building dictionaries...\")\n",
    "dicom_info = pd.read_csv(os.path.join(csv_folder, \"dicom_info.csv\"))\n",
    "\n",
    "# Categorize the image paths\n",
    "full_mammo = dicom_info[dicom_info['SeriesDescription'] == 'full mammogram images']['image_path'].astype(str)\n",
    "cropped_images = dicom_info[dicom_info['SeriesDescription'] == 'cropped images']['image_path'].astype(str)\n",
    "roi_img = dicom_info[dicom_info['SeriesDescription'] == 'ROI mask images']['image_path'].astype(str)\n",
    "\n",
    "# Replace CBIS-DDSM/jpeg with local JPEG directory\n",
    "full_mammo = full_mammo.str.replace(\"CBIS-DDSM/jpeg\", indir, regex=False)\n",
    "cropped_images = cropped_images.str.replace(\"CBIS-DDSM/jpeg\", indir, regex=False)\n",
    "roi_img = roi_img.str.replace(\"CBIS-DDSM/jpeg\", indir, regex=False)\n",
    "\n",
    "# Build dictionaries\n",
    "full_mammo_dict = {path.split(\"/\")[3]: path for path in full_mammo}\n",
    "cropped_images_dict = {path.split(\"/\")[3]: path for path in cropped_images}\n",
    "roi_img_dict = {path.split(\"/\")[3]: path for path in roi_img}\n",
    "\n",
    "print(f\"  full_mammo_dict: {len(full_mammo_dict)} entries\")\n",
    "print(f\"  cropped_images_dict: {len(cropped_images_dict)} entries\")\n",
    "print(f\"  roi_img_dict: {len(roi_img_dict)} entries\")\n",
    "\n",
    "# Function to normalize any CBIS base to local JPEG directory (manually creating path)\n",
    "# The pattern is: ./cbis-ddsm-breast-cancer-image-dataset/jpeg/{UID}/{filename}\n",
    "def _normalize_path(p):\n",
    "    try:\n",
    "        return str(p).replace(\"CBIS-DDSM/jpeg\", indir)\n",
    "    except Exception:\n",
    "        return p\n",
    "\n",
    "# Fallback finder that looks in dicom_info if UID not in dicts\n",
    "_series_map = {\n",
    "    \"full\": \"full mammogram images\",\n",
    "    \"cropped\": \"cropped images\",\n",
    "    \"roi\": \"ROI mask images\",\n",
    "}\n",
    "\n",
    "def find_image_path(original_path: str, series_type: str) -> str:\n",
    "    try:\n",
    "        uid = str(original_path).split(\"/\")[2]\n",
    "        if series_type == \"full\" and uid in full_mammo_dict:\n",
    "            return full_mammo_dict[uid]\n",
    "        if series_type == \"cropped\" and uid in cropped_images_dict:\n",
    "            return cropped_images_dict[uid]\n",
    "        if series_type == \"roi\" and uid in roi_img_dict:\n",
    "            return roi_img_dict[uid]\n",
    "\n",
    "        series_desc = _series_map.get(series_type)\n",
    "        if series_desc is not None:\n",
    "            matches = dicom_info[\n",
    "                (dicom_info['SeriesDescription'] == series_desc) &\n",
    "                (dicom_info['image_path'].astype(str).str.contains(uid, na=False))\n",
    "            ]\n",
    "            if not matches.empty:\n",
    "                return _normalize_path(matches.iloc[0]['image_path'])\n",
    "\n",
    "        return _normalize_path(original_path)\n",
    "    except Exception:\n",
    "        return _normalize_path(original_path)\n",
    "\n",
    "# Gather target case_description CSVs\n",
    "csv_files = glob.glob(os.path.join(csv_folder, \"*.csv\"))\n",
    "case_files = [f for f in csv_files if 'case_description' in f and '_cleaned' not in f]\n",
    "\n",
    "print(\"Found case description files to process:\")\n",
    "for file in case_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Process each case description file\n",
    "for csv_file in case_files:\n",
    "    print(f\"\\nProcessing {os.path.basename(csv_file)}...\")\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"  Original shape: {df.shape}\")\n",
    "\n",
    "    required_columns = ['image file path', 'cropped image file path', 'ROI mask file path']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(f\"  Skipping {os.path.basename(csv_file)} - missing required columns\")\n",
    "        continue\n",
    "\n",
    "    failed_rows = []\n",
    "\n",
    "    # Apply path fixing\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            df.at[index, 'image file path'] = find_image_path(row['image file path'], \"full\")\n",
    "            df.at[index, 'cropped image file path'] = find_image_path(row['cropped image file path'], \"cropped\")\n",
    "            df.at[index, 'ROI mask file path'] = find_image_path(row['ROI mask file path'], \"roi\")\n",
    "        except Exception as e:\n",
    "            failed_rows.append((index, str(e)))\n",
    "\n",
    "    # Add label column if pathology exists\n",
    "    if 'pathology' in df.columns:\n",
    "        label_map = {\n",
    "            'MALIGNANT': 1,\n",
    "            'BENIGN': 0,\n",
    "            'BENIGN_WITHOUT_CALLBACK': 0,\n",
    "        }\n",
    "        df['label'] = df['pathology'].replace(label_map)\n",
    "        print(\"  Added label column\")\n",
    "\n",
    "    # Save the cleaned file\n",
    "    output_file = csv_file.replace('.csv', '_cleaned.csv')\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"  Saved cleaned file: {os.path.basename(output_file)}\")\n",
    "    print(f\"  Final shape: {df.shape}\")\n",
    "\n",
    "print(\"\\nAll files processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MERGING ALL CLEANED CSV FILES ===\n",
      "Found cleaned files:\n",
      "  - calc_case_description_test_set_cleaned.csv\n",
      "  - calc_case_description_train_set_cleaned.csv\n",
      "  - mass_case_description_test_set_cleaned.csv\n",
      "  - mass_case_description_train_set_cleaned.csv\n",
      "  Shape: (326, 15)\n",
      "  Shape: (1546, 15)\n",
      "  Shape: (378, 15)\n",
      "  Shape: (1318, 15)\n",
      "\n",
      "=== COMBINED DATASET ===\n",
      "Total rows: 3568\n",
      "Total columns: 19\n",
      "\n",
      " Saved combined dataset to: ./cbis-ddsm-breast-cancer-image-dataset/csv\\combined_cleaned.csv\n",
      "\n",
      "First 3 rows of combined dataset:\n",
      "  patient_id  breast density left or right breast image view  abnormality id  \\\n",
      "0    P_00038             2.0                 LEFT         CC               1   \n",
      "1    P_00038             2.0                 LEFT        MLO               1   \n",
      "2    P_00038             2.0                RIGHT         CC               1   \n",
      "\n",
      "  abnormality type             calc type calc distribution  assessment  \\\n",
      "0    calcification  PUNCTATE-PLEOMORPHIC         CLUSTERED           4   \n",
      "1    calcification  PUNCTATE-PLEOMORPHIC         CLUSTERED           4   \n",
      "2    calcification              VASCULAR               NaN           2   \n",
      "\n",
      "                 pathology  subtlety  \\\n",
      "0                   BENIGN         2   \n",
      "1                   BENIGN         2   \n",
      "2  BENIGN_WITHOUT_CALLBACK         5   \n",
      "\n",
      "                                     image file path  \\\n",
      "0  Calc-Test_P_00038_LEFT_CC/1.3.6.1.4.1.9590.100...   \n",
      "1  Calc-Test_P_00038_LEFT_MLO/1.3.6.1.4.1.9590.10...   \n",
      "2  Calc-Test_P_00038_RIGHT_CC/1.3.6.1.4.1.9590.10...   \n",
      "\n",
      "                             cropped image file path  \\\n",
      "0  ./cbis-ddsm-breast-cancer-image-dataset/jpeg/1...   \n",
      "1  ./cbis-ddsm-breast-cancer-image-dataset/jpeg/1...   \n",
      "2  ./cbis-ddsm-breast-cancer-image-dataset/jpeg/1...   \n",
      "\n",
      "                                  ROI mask file path  label  \\\n",
      "0  Calc-Test_P_00038_LEFT_CC_1/1.3.6.1.4.1.9590.1...      0   \n",
      "1  Calc-Test_P_00038_LEFT_MLO_1/1.3.6.1.4.1.9590....      0   \n",
      "2  Calc-Test_P_00038_RIGHT_CC_1/1.3.6.1.4.1.9590....      0   \n",
      "\n",
      "                      source_file  breast_density mass shape mass margins  \n",
      "0  calc_case_description_test_set             NaN        NaN          NaN  \n",
      "1  calc_case_description_test_set             NaN        NaN          NaN  \n",
      "2  calc_case_description_test_set             NaN        NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "# Merge all cleaned CSV files into a single combined file\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"=== MERGING ALL CLEANED CSV FILES ===\")\n",
    "\n",
    "# Get all cleaned CSV files\n",
    "csv_folder = \"./cbis-ddsm-breast-cancer-image-dataset/csv\"\n",
    "cleaned_files = glob.glob(os.path.join(csv_folder, \"*_cleaned.csv\"))\n",
    "\n",
    "print(\"Found cleaned files:\")\n",
    "for file in cleaned_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "# Load and combine all cleaned files\n",
    "combined_data = []\n",
    "\n",
    "for file in cleaned_files:\n",
    "    df = pd.read_csv(file)\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    # Add a source column to identify which file each row came from\n",
    "    source_name = os.path.basename(file).replace(\"_cleaned.csv\", \"\")\n",
    "    df['source_file'] = source_name\n",
    "    \n",
    "    combined_data.append(df)\n",
    "\n",
    "# Combine all dataframes\n",
    "if combined_data:\n",
    "    combined_df = pd.concat(combined_data, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n=== COMBINED DATASET ===\")\n",
    "    print(f\"Total rows: {len(combined_df)}\")\n",
    "    print(f\"Total columns: {len(combined_df.columns)}\")\n",
    "    \n",
    "    \n",
    "    # Save the combined dataset\n",
    "    output_path = os.path.join(csv_folder, \"combined_cleaned.csv\")\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n Saved combined dataset to: {output_path}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nFirst 3 rows of combined dataset:\")\n",
    "    print(combined_df.head(3))\n",
    "    \n",
    "else:\n",
    "    print(\" No cleaned files found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
